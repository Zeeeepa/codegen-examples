name: Comprehensive Testing & Quality Assurance

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '18'
  COVERAGE_THRESHOLD: 90
  PERFORMANCE_THRESHOLD_MS: 500

jobs:
  # Code Quality Checks
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r testing-framework/requirements.txt
          pip install flake8 mypy bandit safety

      - name: Run linting (flake8)
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      - name: Run type checking (mypy)
        run: |
          mypy . --ignore-missing-imports --no-strict-optional

      - name: Run security analysis (bandit)
        run: |
          bandit -r . -f json -o reports/security/bandit-report.json || true
          bandit -r . -f txt

      - name: Check dependencies for vulnerabilities
        run: |
          safety check --json --output reports/security/safety-report.json || true
          safety check

      - name: Upload security reports
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-reports
          path: reports/security/

  # Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12', '3.13']
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r testing-framework/requirements.txt

      - name: Create reports directory
        run: mkdir -p reports/coverage reports/test-results

      - name: Run unit tests with coverage
        run: |
          cd testing-framework
          python -m pytest unit-tests/ \
            --cov=. \
            --cov-report=html:../reports/coverage/html \
            --cov-report=xml:../reports/coverage/coverage.xml \
            --cov-report=json:../reports/coverage/coverage.json \
            --cov-report=term-missing \
            --html=../reports/test-results/unit-tests.html \
            --self-contained-html \
            --json-report \
            --json-report-file=../reports/test-results/unit-tests.json \
            --maxfail=10 \
            -v

      - name: Check coverage threshold
        run: |
          python -c "
          import json
          with open('reports/coverage/coverage.json') as f:
              data = json.load(f)
          coverage = data['totals']['percent_covered']
          threshold = ${{ env.COVERAGE_THRESHOLD }}
          print(f'Coverage: {coverage:.2f}%, Threshold: {threshold}%')
          if coverage < threshold:
              print(f'❌ Coverage {coverage:.2f}% is below threshold {threshold}%')
              exit(1)
          else:
              print(f'✅ Coverage {coverage:.2f}% meets threshold {threshold}%')
          "

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: reports/coverage/coverage.xml
          flags: unit-tests
          name: codecov-umbrella

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: unit-test-results-${{ matrix.python-version }}
          path: reports/

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_db
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r testing-framework/requirements.txt
          pip install docker-compose

      - name: Start test environment
        run: |
          cd testing-framework/integration-tests/environments
          docker-compose -f docker-compose.test.yml up -d
          sleep 30  # Wait for services to be ready

      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db
          REDIS_URL: redis://localhost:6379
        run: |
          cd testing-framework
          python -m pytest integration-tests/ \
            --html=../reports/test-results/integration-tests.html \
            --self-contained-html \
            --json-report \
            --json-report-file=../reports/test-results/integration-tests.json \
            -v \
            -m "integration"

      - name: Stop test environment
        if: always()
        run: |
          cd testing-framework/integration-tests/environments
          docker-compose -f docker-compose.test.yml down

      - name: Upload integration test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: reports/test-results/

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    if: github.event_name == 'push' || github.event_name == 'schedule'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r testing-framework/requirements.txt

      - name: Run performance benchmarks
        run: |
          cd testing-framework
          python -m pytest performance-tests/benchmark_tests/ \
            --benchmark-only \
            --benchmark-json=../reports/performance/benchmark-results.json \
            --benchmark-html=../reports/performance/benchmark-results.html

      - name: Run load tests (short)
        run: |
          cd testing-framework/performance-tests/load_tests
          # Run a short load test for CI
          locust -f locust_workflows.py --headless \
            --users 10 --spawn-rate 2 --run-time 60s \
            --host http://localhost:8000 \
            --html ../../../reports/performance/load-test-results.html

      - name: Check performance thresholds
        run: |
          python -c "
          import json
          with open('reports/performance/benchmark-results.json') as f:
              data = json.load(f)
          
          threshold_ms = ${{ env.PERFORMANCE_THRESHOLD_MS }}
          failed_tests = []
          
          for benchmark in data['benchmarks']:
              mean_time_ms = benchmark['stats']['mean'] * 1000
              if mean_time_ms > threshold_ms:
                  failed_tests.append(f\"{benchmark['name']}: {mean_time_ms:.2f}ms\")
          
          if failed_tests:
              print('❌ Performance tests failed:')
              for test in failed_tests:
                  print(f'  - {test}')
              exit(1)
          else:
              print('✅ All performance tests passed')
          "

      - name: Upload performance results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: performance-test-results
          path: reports/performance/

  # Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r testing-framework/requirements.txt

      - name: Run security tests
        run: |
          cd testing-framework
          python -m pytest security-tests/ \
            --html=../reports/security/security-tests.html \
            --self-contained-html \
            -v \
            -m "security"

      - name: Run OWASP ZAP baseline scan
        uses: zaproxy/action-baseline@v0.7.0
        with:
          target: 'http://localhost:8000'
          rules_file_name: '.zap/rules.tsv'
          cmd_options: '-a'

      - name: Upload security test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: security-test-results
          path: reports/security/

  # Generate Reports
  generate-reports:
    name: Generate Quality Reports
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, performance-tests, security-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Download all artifacts
        uses: actions/download-artifact@v3

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r testing-framework/requirements.txt

      - name: Generate comprehensive report
        run: |
          cd testing-framework
          python quality-metrics/dashboard_generator.py \
            --input-dir ../reports \
            --output-dir ../reports/dashboard

      - name: Upload comprehensive reports
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-quality-report
          path: reports/dashboard/

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            // Read test results
            let comment = '## 🧪 Test Results Summary\n\n';
            
            try {
              // Add coverage info
              const coverage = JSON.parse(fs.readFileSync('reports/coverage/coverage.json', 'utf8'));
              const coveragePercent = coverage.totals.percent_covered;
              const coverageEmoji = coveragePercent >= 90 ? '🟢' : coveragePercent >= 80 ? '🟡' : '🔴';
              comment += `${coverageEmoji} **Coverage**: ${coveragePercent.toFixed(2)}%\n`;
              
              // Add test counts
              const unitTests = JSON.parse(fs.readFileSync('reports/test-results/unit-tests.json', 'utf8'));
              comment += `✅ **Unit Tests**: ${unitTests.summary.passed} passed, ${unitTests.summary.failed} failed\n`;
              
              // Add performance info if available
              try {
                const perf = JSON.parse(fs.readFileSync('reports/performance/benchmark-results.json', 'utf8'));
                comment += `⚡ **Performance**: ${perf.benchmarks.length} benchmarks completed\n`;
              } catch (e) {
                comment += `⚡ **Performance**: Tests skipped\n`;
              }
              
              comment += '\n📊 [View detailed reports in artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})';
              
            } catch (error) {
              comment += '❌ Error reading test results';
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Quality Gates
  quality-gates:
    name: Quality Gates
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests]
    if: always()
    steps:
      - name: Check quality gates
        run: |
          echo "Checking quality gates..."
          
          # This would typically check:
          # - Coverage threshold
          # - Test pass rate
          # - Security scan results
          # - Performance benchmarks
          
          # For now, we'll check if previous jobs succeeded
          if [[ "${{ needs.code-quality.result }}" != "success" ]]; then
            echo "❌ Code quality checks failed"
            exit 1
          fi
          
          if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then
            echo "❌ Unit tests failed"
            exit 1
          fi
          
          if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then
            echo "❌ Integration tests failed"
            exit 1
          fi
          
          echo "✅ All quality gates passed"

